{"name":"Ctltest","tagline":"Collection of tests for OpenFlow controllers testing","body":"Collection of scripts for OpenFlow controllers testing using Cbench tool\r\n\r\nJune, 2013\r\n\r\n\r\nDescription\r\n=======\r\n  \r\n  Ctltest is a collection of scripts for performance benchmarking OpenFlow controllers.\r\n  The scripts use Cbench tool to measure controllers' throughput and\r\n  latency. The collection contains scripts for seven popular open source\r\n  OpenFlow controllers, though you can add your own controller to the test.\r\n  \r\n  The scripts install and start the controller on one server and then via SSH \r\n  run Cbench on the other server (you also may use the same server for launching\r\n  the controller and Cbench). Cbench sends Packet In messages to the controller\r\n  and registers the controller's replies (Packet Out or Flow Mod messages).\r\n\r\n  The project contains the following scripts:\r\n\r\n    * Installation and running of popular open source OpenFlow controllers\r\n      (NOX, POX, Floodlight, Trema, Beacon, MuL, Maestro, Ryu)\r\n    * install_contr.sh : script to install all the controllers\r\n    * benchmark_thrpughput.sh : script for performance benchmarking the throughput of the\r\n      controllers with Cbench\r\n    * benchmark_latency.sh : script for performance benchmarking the latency of the \r\n      controllers with Cbench\r\n    * stat.sh : script for collecting statistics of CPU and memory usage\r\n      while running the controllers\r\n    * parser.py : script for parsing the logs of Cbench throughput test\r\n      and plotting the figures via gnulot\r\n      \r\n  All scripts were used for testing controllers under Debian/Ubuntu\r\n  (tested with Ubuntu 12.04 LTS).\r\n\r\n\r\nInstall Controllers\r\n=======\r\n\r\n  To install all the controllers run install_contr.sh script, which takes\r\n  one parameter - the path to the directory where they should be installed.\r\n  \r\n  To install the controllers to the current directory:\r\n\r\n    ./install_contr.sh .\r\n \r\n  \r\nRun Benchmarks\r\n=======\r\n\r\n  First, install the controllers (see \"Install Controllers\" section).\r\n\r\n  To run benchmarks you need to get Cbench tool:\r\n  \r\n  http://docs.projectfloodlight.org/display/floodlightcontroller/Cbench\r\n  \r\n  If you are going to use one server for running the controllers and\r\n  Cbench, install Cbench on the server where you've run install_contr.sh\r\n  script (\"Controllers server\"). Otherwise, install it to another\r\n  server (\"Cbench server\").\r\n\r\n  To benchmark throughput or latency of all the installed controllers\r\n  use benchmark_thrpughput.sh and benchmark_latency.sh scripts.\r\n\r\n  By default it is assumed that you run the controllers on the same host\r\n  with Cbench. If you want to benchmark the controllers running on a\r\n  remote host, change IP adresses of control network interface at the \r\n  Cbnech server and the Controllers server in the both scripts, e.g.:\r\n\r\n    cbench_server=\"username@192.168.1.42\"\r\n    contr_server=\"192.168.1.41\"\r\n\r\n  Note that you need to fill the user name which will be used for SSH\r\n  connection to the Cbench server (also we advise to manage the SSH keys\r\n  on both servers to avoid entering password each time Cbench is run).\r\n  All scripts are run on the Controllers server.\r\n  \r\n  Both scripts take two parameters: the path to the directory where\r\n  the controllers are installed and the duration of one Cbench test.\r\n  If you have installed the controllers into the current directory, run:\r\n  \r\n    ./benchmark_throughput.sh . 10000\r\n    ./benchmark_latency.sh . 10000\r\n\r\n  Each script starts the controllers on the Controller server and then\r\n  runs Cbench on the Cbench server via SSH. Controllers are run with\r\n  different number of avaliable cores. Cbench is run in throughput or\r\n  latency mode and vrying the number of switches and MACs.\r\n  \r\n  The Cbench logs are written to contr_log_th and contr_log_lat.\r\n\r\n  The stat.sh script is run automaticaly in the background. The log is\r\n  written to stats_th and stats_lat.\r\n\r\n  Note that by default Trema controller is not included in the test, as it\r\n  doesn't work properly under Cbench workload with 10 sec test duration.\r\n  To include Trema, add 04_Trema to the CONTR_DIR lists in\r\n  benchmark_throughput.sh and benchmark_latency.sh scripts. You also need to\r\n  set a smaller test duration, e.g.:\r\n  \r\n    ./benchmark_throughput.sh . 5000\r\n    ./benchmark_latency.sh . 5000\r\n\r\n  To plot the results of throughput testing run:\r\n  \r\n    ./parser.py contr_log_th\r\n\r\n    \r\nAdd Your Controller\r\n=======\r\n\r\n  To test your own controller you need to create a directory containing\r\n  the following scripts:\r\n  \r\n    * who.sh : echo the controller's name (for debug purposes)\r\n    * install.sh : script which describes how to install your contrioller\r\n    * start.sh : script for running the controller, can take one parameter - \r\n      the number of CPU cores to use.\r\n  \r\n  For scripts examples see the existing controllers' scripts.\r\n  \r\n  Then add the name of your directory to the CONTR_DIR list in each script:\r\n  install_contr.sh, benchmark_throughput.sh and benchmark_latency.sh.\r\n  Now you can run the scripts as described above, your controller will be\r\n  added to the tests.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}